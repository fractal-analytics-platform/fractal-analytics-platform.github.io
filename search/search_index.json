{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fractal","text":"<p>Fractal is a framework developed at the BioVisionCenter to process bioimaging data at scale in the OME-Zarr format and prepare the images for interactive visualization.</p> <p></p> <p>Fractal enables distributed workflows that convert TBs of image data into OME-Zarr files. Tasks are modular and can be provided by users to apply image processing and measurements. We provide a package of core processing tasks that include registration, segmentation and measurements. All tasks can be orchestrated by Fractal to run locally or on clusters to scale the image analysis. You can build &amp; control Fractal workflows via the web client or the Fractal command line client.</p> <p>The resulting pyramidal OME-Zarr files enable interactive visualization in different modern viewers like MoBIE and napari. </p>"},{"location":"#fractal-components","title":"Fractal components","text":"<p>Fractal is made of different components, including a server/client architecture, a web client and a set of core tasks for image processing.</p> Component GitHub Repository Documentation Package server fractal-server fractal-server docs fractal-server on PyPI client fractal-client fractal-client docs fractal-client on PyPI web client fractal-web fractal-web docs - core tasks fractal-tasks-core fractal-tasks-core docs fractal-tasks-core on PyPI"},{"location":"#status","title":"Status","text":"<p>Fractal is under active development. We have tasks in our core repository and in multiple other tasks repositories. Our core repository contains a converter for Yokogawa CV7000/CV8000 image data as well as different processing tasks for illumination correction, maximum intensity projection, cellpose segmentation. We're working with others to expand the number of compatible OME-Zarr converters (see e.g. fractal-faim-hcs, which uses the faim-hcs converters for the MD Image Xpress), additional image processing tasks (see e.g. APx Fractal Task Collection for tasks centered around 2D image analysis and drug profiling) and additional processing tasks (e.g. the scMultipleX package, which contains a Fractal task to make many scikit-image based measurements in 2D and 3D, as well as organoid registration for multiplexed images). Follow along on the github repositories above and feel free to open issues.</p>"},{"location":"#build-your-own-task","title":"Build your own task","text":"<p>You can easily add your own task to Fractal. Fractal can run Linux executables that follow its task API, as defined in the task building documentation. We primarily run Python-based tasks. You can find a full walk-through and templates in the Build Your Own Fractal Task page.</p>"},{"location":"#examples","title":"Examples","text":"<p>Example datasets and workflows are publicly available:</p> <ul> <li>Example output data from Fractal in the OME-Zarr format can be found here: Small dataset: 10.5281/zenodo.10257149, larger dataset: 10.5281/zenodo.10257532</li> <li>Example input datasets can be found here: Small Fractal dataset for automated testing &amp; task development, Larger example input data for Fractal.</li> <li>Additional example workflows can be found in the fractal-demos repository in the <code>examples</code> folder. </li> </ul>"},{"location":"#contributors-and-license","title":"Contributors and license","text":"<p>Unless otherwise stated in each individual module, all Fractal components are released according to a BSD 3-Clause License, and Copyright is with the BioVisionCenter at the University of Zurich.</p> <p>Fractal was conceived in the Liberali Lab at the Friedrich Miescher Institute for Biomedical Research and in the Pelkmans Lab at the University of Zurich by @jluethi and @gusqgm. The Fractal project is now developed at the BioVisionCenter at the University of Zurich and the project lead is with @jluethi. The core development is done under contract by eXact lab S.r.l..</p>"},{"location":"build_your_own_fractal_task/","title":"Create a Fractal task","text":"<p>Fractal tasks are the core processing units of to build your workflows. Each Fractal task loads the data from one (or many) OME-Zarr(s) and applies processing to them. Fractal tasks are Linux command line executables. For the purpose of this demo, we will look at the Python implementation. You can think of a Fractal task as a Python function that knows how to process an OME-Zarr image and save the results back into that OME-Zarr image. With a bit of syntax sugar, this becomes a Fractal task you can then run from the web interface. To understand the types of tasks, their API &amp; how they provide information to Fractal server, check out the Fractal Tasks Spec page. </p> <p>This page is all about building your own Fractal task. It comes down to 5 steps:  </p> <ol> <li> <p>Create a repository for your tasks using the fractal-tasks-template.  </p> </li> <li> <p>Develop your Python function to process an OME-Zarr as desired &amp; follow the Fractal API for task input &amp; function outputs.  </p> </li> <li> <p>Update the task-list to generate a Fractal manifest in your package.  </p> </li> <li> <p>Package your task (locally or via pypi).  </p> </li> <li> <p>Install your task on a given Fractal server.  </p> </li> </ol> <p>This video walks you through all those steps for how to implement a custom Fractal task that does image-labeling based on a user-defined threshold.</p> <p></p> <p>If you prefer to follow a written guide, follow the instructions in the README of fractal-tasks-template.</p>"},{"location":"fractal_tasks/","title":"Fractal Tasks","text":"<p>(last updated: LASTUPDATEDPLACEHOLDER; <code>fractal-web</code> version: FRACTALWEBREFERENCEPLACEHOLDER)</p>"},{"location":"image_list/","title":"Fractal Image List","text":"<p>While applying a processing workflow to a given dataset, Fractal keeps a list of all the OME-Zarr images it is processing and their metadata. In this page we describe the concepts of images and filters - see also the examples section.</p>"},{"location":"image_list/#images","title":"Images","text":"<p>Each entry in the image list is defined by a unique <code>zarr_url</code> property (the full path to the OME-Zarr image), and it may also include image types and image attributes.</p>"},{"location":"image_list/#image-types","title":"Image types","text":"<p>Image types are boolean properties that allow to split the image list into different sub-lists (e.g. the <code>is_3D</code> type for 3D/2D images, or the <code>illumination_corrected</code> type for raw/corrected images when illumination correction was not run in-place). Types can be set both through the task manifest (e.g. after an MIP task, the resulting images always have the type <code>is_3D</code> set to <code>False</code>) as well as from within an individual task (see task-API/output section).</p> <p>Note: when applying filters to the image list, the absence of a type corresponds to false by default.</p>"},{"location":"image_list/#image-attributes","title":"Image attributes","text":"<p>Image attributes are scalar properties (strings, integers, floats or booleans). They are always defined from within individual tasks, and never by the task manifest. They allow selecting subsets of your data (e.g. select a given well, a given plate or a given multiplexing acquisition).</p> <p>Fractal server uses the image list combined with filters (see below) to provide the right image URLs to each task during execution.</p>"},{"location":"image_list/#filters","title":"Filters","text":"<p>Before running a given task, Fractal prepares an appropriate image list by extracting the images that match with a given set of filters. Filters can refer both to image types or image attributes and they may come from different sources.</p>"},{"location":"image_list/#type-filters","title":"Type filters","text":""},{"location":"image_list/#input-filters","title":"Input filters","text":"<p>The set of type filters to be applied before running a task is obtained by combining these sources:</p> <ol> <li>The dataset may have <code>type_filters</code> set - this is the source with lowest priority.<ul> <li>Example: A prior workflow ran and set output filters of <code>type_filters = {\"is_3D\": False}\"</code>. These output filters were added to the dataset when the prior workflow finished.</li> <li>Example: I manually set <code>type_filters = {\"illumination_corrected\": False}\"</code> through Fractal, by modifying the dataset, because I want to process raw images.</li> </ul> </li> <li>The manifest of a tasks package may specify that a task has some required <code>input_types</code>, which are used as filters.<ul> <li>Example: An \"Project Image (HCS Plate)\" task with <code>input_types={\"is_3D\": True}</code>, meaning that it cannot run on images with type <code>is_3D=False</code>.</li> <li>Example: An \"Illumination correction\" task with <code>input_types={\"illumination_corrected\": False}</code>, meaning that it cannot run on images with type <code>illumination_correction=True</code>.</li> <li>Example: An \"Apply Registration to Image\" task with <code>input_types={\"registered\": False}</code>, meaning that it cannot run on images with type <code>registered=True</code>.</li> </ul> </li> <li>For a task within a workflow, it is possible to specify some additional <code>type_filters</code>.<ul> <li>Example: I may need a workflow that includes a 3D-&gt;2D projection task but then switches back to 3D images in a later task. I can achieve this by setting <code>type_filters = {\"is_3D\": True}</code> for the relevant task, so that from this task onwards the 3D images are processed (and not the 2D ones).</li> </ul> </li> </ol>"},{"location":"image_list/#output-filters","title":"Output filters","text":"<p>The task manifest may include the <code>output_types</code> property for a specific task. If this is the case, then these types are both applied to all output images and they are included in the dataset <code>type_filters</code>.</p> <p>Examples:</p> <ul> <li>A 3D-&gt;2D projection task typically has <code>output_types = {\"is_3D\": False}</code>: from this task onwards, the 2D images are processed (not the raw 3D images). And the images generated by this task have their type set to <code>\"is_3D\": False</code>.</li> <li>An illumination-correction task would have <code>output_types = {\"illumination_corrected\": True}</code>: from this task onwards, the illumination corrected images are processed (not the raw images). And the images generated by this task have their type set to <code>\"illumination_corrected\": True</code>.</li> </ul>"},{"location":"image_list/#attribute-filters","title":"Attribute filters","text":"<p>The set of attribute filters to be applied before running a task is defined upon submission of a job, and they do not change during the job execution. These filters offer a way to process a subset of the whole dataset (e.g. only a few wells rather than the whole plate, or only a given multiplexing acquisition cycle). In Fractal web, the \"Continue Workflow\" dialogue is prepopulated with the attribute filters from the dataset (if any are set), but users are able to change the attribute filters to any setting they want.</p>"},{"location":"image_list/#examples","title":"Examples","text":"<p>After running a converter task, I may have an OME-Zarr HCS plate with 2 wells that contain one image each. In this case, the image list has 2 entries and each image has attributes for plate and well, as well as a true/false <code>is_3D</code> type.</p> <p></p> <p>If I then run an illumination-correction task that does not overwrite its input images, the image list includes the two original images (without the <code>illumination_corrected</code> type) and two new ones (with <code>illumination_corrected</code> type set to true). Note that this task also sets the dataset type filters to <code>{\"illumination_correction\": True}</code>.</p> <p></p> <p>If I then run an MIP task, this will act on the two images with <code>illumination_corrected</code> set to true, due to the dataset filters. After the task has run, two new images are added to the list (with type <code>is_3D</code> set to false).</p> <p></p> <p>Another example is that if I have an OME-Zarr HCS plate with 3 wells and each well has 3 multiplexing acquisition, then the image list includes 9 OME-Zarr images (and those entries should have the acquisition attribute set).</p>"},{"location":"run_fractal/","title":"Deploy Fractal","text":"<p>Fractal is meant to be deployed to manage workflows on large cluster and currently has support for different modes of running on slurm clusters. It is deployed on Linux servers and also runs on macOS or Windows (by using Windows Subsystem Linux). You can run a fully containerized local example that is useful for demos and testing purposes by following the instructions in the fractal containers repository or by following along this walkthrough:</p> <p></p> <p>More detailed documentation about the configuration of the different Fractal components can be found in the <code>fractal-server</code> documentation and the <code>fractal web</code> documentation.</p> <p>Fractal can also be deployed by manually setting up the server in a Python environment, configuring your own postgres database &amp; setting up Fractal web from source. You can find some helpful material for this in the  fractal-demos repository (especially the examples/server section). We also have older video walkthroughs on manual setups available for both the fractal-server as well as fractal-web.</p>"},{"location":"tasks_spec/","title":"Fractal Tasks Specification","text":"<p>Fractal tasks are modular and interoperable processing units that handle data in OME-Zarr containers. Each task is an executable that runs on a single OME-Zarr image or a collection of OME-Zarr images. In Fractal, we the OME-Zarrs to be processed by giving the tasks the zarr_urls(s), the paths to a given OME-Zarr image on disk or in the cloud. All tasks load data from an OME-Zarr on disk and store their processing results in an OME-Zarr (the same or a new one) on disk again. The parameters and metadata of tasks are described in a Fractal manifest in json form. This page contains an overview of the Fractal task specification, the types of Fractal tasks, the manifest that specifies task metadata as well as their input &amp; output API.</p> <p></p>"},{"location":"tasks_spec/#task-types","title":"Task Types","text":"<p>There are three types of tasks in Fractal V2: parallel tasks, non-parallel tasks &amp; compound tasks.</p> <ol> <li>A parallel task is written to process a single OME-Zarr image and meant to be run in parallel across many OME-Zarr images. <ul> <li>Parallel tasks are the typical scenario for compute tasks that don't need special input handling or subset parallelization. </li> <li>Parallel tasks can typically be run on any collection of OME-Zarrs.</li> </ul> </li> <li>A non-parallel task processes a list of images, and it only runs as a single job. <ul> <li>Non-parallel tasks are useful to aggregate information across many OME-Zarrs or to create image-list updates (see the Fractal image list).</li> <li>Non-parallel tasks can often be specific to given collection types like OME-Zarr HCS plates.</li> </ul> </li> <li>A compound task consists of an initialization (non-parallel) task and a (parallel) compute task.<ul> <li>The initialization task runs in the same way as a non-parallel task and generates a custom parallelization list of zarr_urls &amp; parameters to be used in the compute task.</li> <li>The compute tasks are run in parallel for each entry of the parallelization list and use the <code>init_args</code> dictionary as an extra input from the initialization task.</li> <li>Compound tasks can often be specific to given collection types like OME-Zarr HCS plates. A typical example are multiplexing-related tasks that use <code>acquisition</code> metadata on the well level to decide which pairs of images need to be processed.</li> </ul> </li> </ol>"},{"location":"tasks_spec/#task-list-and-manifest","title":"Task list and manifest","text":"<p>A package that provides Fractal tasks must contain a manifest (stored as a <code>__FRACTAL_MANIFEST__.json</code> file within the package), that describes the parameters, executables and metadata of the tasks. <code>fractal-tasks-core</code> and <code>fractal-tasks-template</code> offer a simplified way to generate this manifest, based on a task list written in Python. </p>"},{"location":"tasks_spec/#task-list","title":"Task list","text":"<p>If the task package <code>my-pkg</code> was created based on the template, the task list is in <code>src/my-pkg/dev/task_list.py</code> and includes entries like <pre><code>TASK_LIST = [\n    NonParallelTask(\n        name=\"My non-parallel task\",\n        executable=\"my_non_parallel_task.py\",\n        meta={\"cpus_per_task\": 1, \"mem\": 4000},\n        category=\"Conversion\",\n        docs_info=\"file:task_info/task_description.md\",\n        tags=[\"tag1\", \"Microscope name\"]\n    ),\n    ParallelTask(\n        name=\"My parallel task\",\n        executable=\"my_parallel_task.py\",\n        meta={\"cpus_per_task\": 1, \"mem\": 4000},\n        category=\"Segmentation\",\n    ),\n    CompoundTask(\n        name=\"My compound task\",\n        executable_init=\"my_task_init.py\",\n        executable=\"my_actual_task.py\",\n        meta_init={\"cpus_per_task\": 1, \"mem\": 4000},\n        meta={\"cpus_per_task\": 2, \"mem\": 12000},\n        category=\"Registration\",\n    ),\n]\n</code></pre> where the different task models refer to the different task types. Given such task list, running the following command <pre><code>python src/my-pkg/dev/create_manifest.py\n</code></pre> generates a JSON file with the up-to-date manifest. Note that advanced usage may require minor customizations of the create-manifest script.</p>"},{"location":"tasks_spec/#manifest-metadata","title":"Manifest metadata","text":"<p>The task manifest can contain additional metadata that makes it easier for people to browse tasks on the Fractal task page and the tasks available on a given server. The Fractal task template provides good defaults for how all this metadata can be set. This metadata is also used to make tasks searchable.</p>"},{"location":"tasks_spec/#docs-info","title":"Docs info","text":"<p>Tasks can provide a structured summary of their functionality. If the image list does not contain a docs_info property for a given task, the docstring of the task function is used. A developer can provide a more structured markdown file by specifying the relative path to the markdown file with the task description (for example: <code>file:task_info/task_description.md</code>). The convention for these task descriptions is to contain a section on the purpose of the task as well as its limitations in a bullet-point list.</p>"},{"location":"tasks_spec/#categories","title":"Categories","text":"<p>Tasks can belong to a single category, which allows users to filter for the kind of task they are looking for. The standard categories are: <code>Conversion</code>, <code>Image Processing</code>, <code>Segmentation</code>, <code>Registration</code>, <code>Measurement</code>.</p>"},{"location":"tasks_spec/#modalities","title":"Modalities","text":"<p>Tasks can have a single modality metadata. If a task works on all types of OME-Zarrs, no modality should be set. If a task is specifically designed to work on one modality (for example, a task that required OME-Zarr HCS plates), the modality should be specified. The standard modalities are: <code>HCS</code>, <code>lightsheet</code>, <code>EM</code>.</p>"},{"location":"tasks_spec/#tags","title":"Tags","text":"<p>Tasks can have arbitrary lists of string tags that describe their functionality. These are particularly helpful to increase the findability of a task using search.</p>"},{"location":"tasks_spec/#authors","title":"Authors","text":"<p>Task packages can specify an authors list. This metadata is configured in the create_manifest.py script for the whole task package.</p>"},{"location":"tasks_spec/#how-to-get-your-task-package-on-the-fractal-tasks-page","title":"How to get your task package on the Fractal tasks page","text":"<p>If you have a task package that you would like to see listed on the Fractal task page page, ping one of the Fractal maintainers about it or make a PR to have your task included in the list of task sources here. For a task package to be listable on the Fractal tasks page, the package needs to contain a Fractal manifest and be available either via PyPI or via a whl in Github releases. The Fractal task template provides examples for how to do both. Future work will add support for adding additional task configurations (likely a specification for how to provide packages that are installable via Pixi).</p>"},{"location":"tasks_spec/#input-api","title":"Input API","text":""},{"location":"tasks_spec/#parallel-tasks","title":"Parallel tasks","text":"<p>The input arguments of a Fractal parallel tasks must include a <code>zarr_url</code> string argument. The <code>zarr_url</code> contains the full path to the zarr file to be processed. Only filesystem paths are currently supported, not S3 urls. <code>zarr_url</code> is a reserved keyword argument: when running tasks through Fractal server, the server takes care to pass the correct <code>zarr_url</code> argument to the parallel task (based on filtering the image list). Tasks can also take an arbitrary list of additional arguments that are specific to the task function and that the user can set.</p>"},{"location":"tasks_spec/#non-parallel-tasks","title":"Non-parallel tasks","text":"<p>The input arguments of a Fractal non-parallel task must include a <code>zarr_urls</code> arguments (a list of strings) and <code>zarr_dir</code> argument (a single string). <code>zarr_urls</code> contains the full paths to the OME-Zarr images to be processed. We currently just support paths on filesystems, not S3 urls. <code>zarr_dir</code> is typically the base directory into which OME-Zarr files will be written by tasks and it is mostly used by converters. Both <code>zarr_urls</code> and <code>zarr_dir</code> are reserved keyword arguments: when running tasks through Fractal server, the server takes care to pass the correct filtered list <code>zarr_urls</code> and the correct <code>zarr_dir</code> to the non-parallel task. Tasks can also take an arbitrary list of additional arguments that are specific to the task function and that the user can set.</p>"},{"location":"tasks_spec/#compound-tasks","title":"Compound tasks","text":"<p>Compound tasks consist of an init part (similar to the non-parallel task) and a compute part (similar to the parallel task). The init part has the same Input API as the non-parallel task (<code>zarr_urls</code> and <code>zarr_dir</code>), but it provides the parallelization list for the compute part as an output. The compute part takes the <code>zarr_url</code> argument and an extra <code>init_args</code> dictionary argument (which is coming from the <code>parallelization_list</code> provided by the init task).</p>"},{"location":"tasks_spec/#output-api","title":"Output API","text":"<p>Tasks can optionally return updates to the image list (this is true for all tasks except the init phase of a compound tasks) or a parallelization list (just the init phase of a compound task). The output of a task is always a <code>task_output</code> dictionary. Note that this dictionary must be JSON-serializable, since it will be written to disk so that <code>fractal-server</code> can access it.</p> <p>For tasks that create new images or edit relevant image properties, <code>task_output</code> must include an <code>image_list_updates</code> property so the server can update its metadata about that image.</p> <p>NOTE: if both <code>image_list_updates</code> and <code>image_list_removals</code> are empty in the task output, then <code>fractal-server</code> includes all the filtered image list into <code>image_list_updates</code>, so that they are updated with the appropriate <code>types</code> (see also the image-list page).</p> <p>Task outputs with image list updates are returned as a dictionary that contains the <code>image_list_updates</code> key and a list containing the updates to individual images. The updates need to be for unique <code>zarr_url</code>s and each update needs to contain the <code>zarr_url</code> of the image it\u2019s providing an update for. Additionally, they can provide an <code>origin</code> key, an <code>attributes</code> key and a <code>types</code> key. The <code>origin</code> key describes the <code>zarr_url</code> of another image already in the image list and will take the existing attributes and types from that image. Attributes and types can also be directly set by a task.</p> <p>Here's an example of <code>task_output</code>: <pre><code>{\n    \"image_list_updates\" = [\n        {\n            \"zarr_url\": \"/path/to/my_zarr.zarr/B/03/0_processed\",\n            \"origin\": \"/path/to/origin_zarr.zarr/B/03/0\",\n            \"attributes\": {\n                \"plate\": \"plate_name\",\n                \"well\": \"B03\"\n            },\n            \"types\": {\n                \"is_3D\": True\n            }\n        }\n    ]\n}\n</code></pre></p> <p>The init part of a compound task must produe a parallelization lists, with elements having the <code>zarr_url</code> property as well as additional arbitrary arguments as an <code>init_args</code> dictionary. Parallelization lists are provided in the following structure: <pre><code>{\n    \"parallelization_list\": [\n        {\n            \"zarr_url\": \"/path/to/my_zarr.zarr/B/03/0\",\n            \"init_args\": {\"some_arg\": \"some_value\"},\n        }\n    ]\n}\n</code></pre></p>"},{"location":"news/","title":"News","text":"<p>Welcome to the Fractal news page! Every few months we\u2019ll post updates here about new features, releases, and important milestones.</p> <ul> <li>2025-04-28: Fractal Flexibility   We launched Fractal flexibility to enable new modes of workflow submission and monitoring.</li> <li>2025-12-17: Fractal Project Sharing We added project sharing and improved visualisations in web viewers like ViZarr through new fractal-data features.</li> </ul>"},{"location":"news/2025-04-28-fractal-flexibility/","title":"Fractal flexibility","text":"<p>The Fractal framework, developed at the BioVisionCenter, allows users to build complex image processing workflows and run them on Terabytes of high-dimensional microscopy data. </p> <p>As our user base has grown over last year, so has the need to test workflows on small subsets of data and trace exactly where issues occur. For this purpose, we\u2019ve invested a lot of our work over the past months in what we call the \u201cFractal flexibility project\u201d. </p> <p>The Fractal server 2.14 update enables more flexible filtering in workflows, allows new modes of submitting workflows in parts and monitoring their progress more dynamically, and improves our underlying history data structure to ensure we keep track of granular processing steps. This post explains how filters &amp; history work in the Fractal framework and what new modes the recent Fractal updates have enabled for running more interactive workflows.</p>"},{"location":"news/2025-04-28-fractal-flexibility/#flexible-filters-in-the-fractal-framework","title":"Flexible filters in the Fractal framework","text":"<p>When processing OME-Zarrs with Fractal, the framework keeps track of all the images imported or created in the image list. These images contain metadata about the type of images (e.g., are they 2D or 3D images, were they processed by an illumination correction or registration task, etc.), as well as attributes that describe how the image fits into the larger collection. In a high content screening context, that often means which plate and well the image belongs to, or whether it is one of multiple image acquisitions.</p> <p></p> <p>In Fractal workflows, every task gets a filtered selection of the image list to process and can provide updates to this image list, be they new images that get added or changes to the attributes and types of existing entries. The image list keeps track of all the available images, while each task gets a precise selection of images to process. </p> <p></p> <p>Using type filters allows splitting datasets into different sub-lists. This allows the design of complex workflows, for instance converting 3D data to an OME-Zarr, making a 2D projection, and then analyzing only the 2D data for multiple processing steps until some of the results get mapped back into 3D space. Attribute filters on the other hand allow to select subsets of a given dataset, for example selecting a given well of a multi-well plate or processing only one acquisition out of many in a multiplexed imaging setup.</p> <p>In order to allow more advanced filters, we have overhauled our filter system from fractal-server 2.11 and earlier. The first change was a move away from datasets keeping a state of type and attribute filters. Fractal-server 2.14 and onwards now allow setting filters during workflow submission instead, making the filters that are used more transparent and enabling running full workflows on subsets of a dataset instead of running every task on the whole dataset. </p> <p>We also moved away from setting dynamic filters based on task code. While type filters can be pre-set by the task manifest (e.g., the projection task requires 3D input data and produces 2D output data. After the projection task, the workflow will default to process the 2D images as specified in its task manifest) or set by the user in a workflow (e.g. a user specifies that they want to switch back to 3D data at a given task in the workflow), they can no longer be dynamically set from within the task code. This more static definition of type filters simplifies the dynamic resubmission of partial workflows without relying on a dataset state.</p> <p>Finally, we have made attribute filters more powerful. Instead of setting simple matching filters (e.g., \u201cis the well attribute exactly B03\u2019\u201d), fractal-server 2.12 and onwards allow to filter for more complex subsets (e.g., \u201cis the well in the list of wells [B03, B04, B05]?\u201d).</p>"},{"location":"news/2025-04-28-fractal-flexibility/#fractals-granular-history","title":"Fractal\u2019s granular history","text":"<p>Once we had both simplified and expanded the scope of filters, we wanted to allow users to submit complex workflows on subsets of their data. This will allow running any part of a workflow in any order and monitor the dynamic status of workflows. In order to do that, we needed an overhaul of our history data structure. Specifically, we needed to expand the history structure to store more granular information about which subset of the dataset was run with which tasks using what parameters. </p> <p>Fractal allows for different types of tasks to be run. While parallel tasks run once per image, we also support non-parallel tasks, compound tasks and converter tasks, which have been newly added as a separate category. Non-parallel tasks run on multiple images with the same parameters in a single execution. Converters don\u2019t have any input OME-Zarrs, as they are the tasks first creating them. Compound tasks are the most complex, allowing task developers to come up with their own parallelisation schemes and run anything from fewer jobs than images, to parallelise with a job for each image or even run multiple jobs for each image (e.g. one per time point in a time series). Our new history data structure needed to reflect this complexity and give users easy access to the logs of what was run for each image.</p> <p>Additionally, the scalability of our history data structure was a concern. There were inherent trade-offs between our old simplistic system of keeping a history of how every task was run with its parameters (one entry per task and full dataset run on it) and a system that focuses on storing individual full histories for each image. In the naive way of storing all parameters that were used when processing each image, our history database would scale very poorly with workflows processing thousands or tens of thousands of images, where the same parameters are often reused to process all images. </p> <p>Given the complexities of the different task types and the scalability of the data structure, we came up with a new history system based on \u201cRuns\u201d. Every time a task is run on a selection of images, this is saved as a \u201cRun\u201d of that task. Each run can process as many images of the image list as were filtered (a single image or tens of thousands of images). The run keeps track of what parameters were used to process which images. Within a run, we can then keep track of whether the run succeeded for a given unit (a given execution within a run, e.g. a single image being processed by a parallel task) and make the granular logs directly accessible in the web interface. </p> <p></p>"},{"location":"news/2025-04-28-fractal-flexibility/#new-modes-of-submitting-monitoring-workflows","title":"New modes of submitting &amp; monitoring workflows","text":"<p>Using our refactored filter logic and the improved history data structure, we could enable the more complex workflow submission and monitoring patterns we were aiming for.</p> <p>The first feature we added is a much more accessible way of submitting workflows on subsets of your dataset. This is particularly useful when one has large datasets and wants to test a workflow on a small subset of just a few images, or when one step in the workflow needs to be submitted with different parameters for subsets of the dataset. At workflow submission, the user is now shown the pre-filtered image list and gets to adapt the filters to exactly what they want to run, before submitting the workflow.  </p> <p></p> <p>While flexible submission is great, it\u2019s only as useful as our ability to monitor and record what has happened. Thanks to the improved history data structure, we can now keep track of complex workflow submission patterns and display the granular status of your dataset directly on the workflow page. This allows a user to see both the aggregated status of a given task (e.g., \u201chow many of my images have been processed by task X?\u201d) as well as dive into a detailed view to browse the image list with associated processing statuses and logs for that task (e.g., \u201cwhat\u2019s the status of the image in well B03, acquisition 2?\u201d).  </p> <p></p> <p>With this newly added flexibility, Fractal makes it easier to manage complex workflows, test parameters on small subsets and find specific images that had issues during processing. This update lays the groundwork for more improvements to come in the future. Among the key features still on our roadmap, we want to highlight 3 relevant ones:  </p> <ol> <li> <p>Improved visibility into the processing status of compound tasks: Compound tasks in Fractal combine a non parallel initialization phase that decides on the parallelisation of processing with running these parallel compute jobs. Because images can be processed in multiple compute tasks or even skipped completely from compute tasks based on the initialization, we only track the image status aggregated on all images that were run at once. Future refactors of the compound tasks may allow us to define a subset of compound tasks for which we\u2019ll be able to show more granular processing status, without having to remove the flexibility for complex compound tasks that are not run on individual images.  </p> </li> <li> <p>More granular live status updates: While the new history data structure allows granular tracking of statuses, our current slurm runners only update them in batches. We run multiple images within the same slurm job (this batching has large performance gains) and only update the status of all images once the slurm job is done. Future work on refactoring the runners may enable us to make the status update more granular.  </p> </li> <li> <p>Rerun workflows based on status: While we can now track processing status per image, we haven\u2019t exposed a submission mode that allows rerunning a workflow only on failed images or running a workflow on all the images that were not run yet. This feature is high on our priority list and will be tackled after this release.  </p> </li> </ol> <p>You can follow along with our ongoing Fractal developments by keeping an eye on this newly created news section on the Fractal page or by following the development process on our Github repositories.  </p>"},{"location":"news/2025-12-17-fractal-project-sharing/","title":"Fractal Project Sharing","text":"<p>The Fractal framework, developed at the BioVisionCenter, allows users to build complex image processing workflows and run them on Terabytes of high-dimensional microscopy data. </p> <p>With more people using Fractal and collaborating on analysis workflows, we wanted to bring more collaboration features to our users. As a first such feature, we have recently added project sharing functionality to Fractal with the 2.18 Fractal server update.</p> <p>Project sharing allows users within a given Fractal server to collaborate granularly on their projects. Users can give access for collaborators to see their projects, stream the processed image data or collaborate on building and running workflows.</p> <p>The ability for project sharing comes together with an improved fractal-data service. Fractal-data allows authenticated streaming of OME-Zarrs to viewers like ViZarr and analysis environments like the Fractal feature explorer. As part of this effort, we have simplified the authentication logic to follow project directories &amp; dataset Zarr directories. This allowed us to add support for data streaming of any datasets in shared projects: If your colleague processed OME-Zarrs in a project shared with you, you now also have access to visualise the images directly in a web viewer like ViZarr or browse the measurements in the Fractal feature explorer.</p> <p></p> <p>To enable these new data streaming services, we've revamped the project directory logic: Fractal used to default to put your OME-Zarrs in your (single) project directory. With the new update, we now enforce that all Fractal output go into a project directory. This both simplified the dataset creation options (users don't need to know the base paths to the zarr directory anymore to modify the target location of their dataset zarr directories) and ensures that we can safely use project directories for streaming authentication. To handle complex use-cases, we now support multiple project directories for a single user.</p> <p>TODO: Image sharing options </p> <p>We wanted to support a variety of use-cases for project sharing: from easily showing images and quantification results to a colleague in the lab or collaborating on creating workflows, to members of core facilities supporting users by setting up initial workflows for them or helping them with existing workflows in their projects. To enable this, project sharing permissions are granular:  - Share a project in read-only mode to gives access to view the workflows and stream the OME-Zarrs (see above): This ensures your collaborator can see how you processed your data and can see the data itself. - Share a project in read + write mode to allow editing workflows: Want your collaborator to help with workflow parameters? This gives them access to modify your workflow. - Share a project in read + write + execute mode: This gives full access to your collaborator. On top of everything above, they can now also run workflows from the shared project. Share workflow execution comes with some limitations depending on your deployments: Both users need to have data access within the linux cluster environment to modify the same files. In ssh-slurm deployments (like the UZH Science Cluster &amp; UZH Science Cloud servers), this typically holds for users in the same research group, but not across research groups. In sudo-slurm deployments, this depends on configurations (please contact an admin for questions). </p> <p>You can follow along with our ongoing Fractal developments by keeping an eye on this newly created news section on the Fractal page or by following the development process on our Github repositories.  </p>"}]}